{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7139d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import cv2\n",
    "import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f732ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the other notebook\n",
    "experiments = {\n",
    "    \"fantasy\": (\"./data/fantasy\", \"checkpoints/UNet128_2025-09-12_08-24-21_fantasy.pth\"),\n",
    "    \"cats_and_dogs\": (\"./data/cats_and_dogs/dataset/training_set\", \"checkpoints/UNet128_2025-09-08_17-57-30_cats_and_dogs.pth\"),\n",
    "    \"pokemon\": (\"./data/pokemon_archive\",\"UNet128_2025-09-12_11-03-12_pokemon.pth\"),\n",
    "    \"dragon\": (\"utils/dataset_fetcher/datasets/\", \"checkpoints/UNet128_2025-09-12_19-47-15_dragon.pth\"),\n",
    "    \"yugioh\": (\"data/yugioh/YuGiOhImages\",None),\n",
    "    \"mnist\": (None,None),\n",
    "}\n",
    "HR=256\n",
    "LR=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434890c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterCropSquare:\n",
    "    def __call__(self, img: Image):\n",
    "        w, h = img.size\n",
    "        min_side = min(w, h)\n",
    "        left = (w - min_side) // 2\n",
    "        top = (h - min_side) // 2\n",
    "        right = left + min_side\n",
    "        bottom = top + min_side\n",
    "        return img.crop((left, top, right, bottom))\n",
    "\n",
    "\n",
    "class PadToSquare:\n",
    "    def __init__(self, mode=\"both\", color: tuple|None = None, size=None):\n",
    "        assert mode in ['both', 'w', 'h'], \"Mode must be 'both', 'width', or 'height'\"\n",
    "        self.mode = mode\n",
    "        self.color = color\n",
    "        self.size = size\n",
    "    def __call__(self, img: Image):\n",
    "        w, h = img.size\n",
    "        np_img = np.array(img)\n",
    "        if not self.color:\n",
    "            avg_color = tuple(np_img.mean(axis=(0, 1)).astype(int))\n",
    "        clr = self.color or avg_color\n",
    "        size = self.size or max(h,w)\n",
    "        pad_w = size - w if (size > w and self.mode != 'h') else 0\n",
    "        pad_h = size - h if (size > h and self.mode != 'w') else 0\n",
    "        pad_l = pad_w // 2\n",
    "        pad_r = pad_w - pad_l\n",
    "        pad_t = pad_h // 2\n",
    "        pad_b = pad_h - pad_t\n",
    "        padding = (pad_l, pad_t, pad_r, pad_b) \n",
    "        return ImageOps.expand(img, padding, fill=clr)  # fill=0 for black padding\n",
    "\n",
    "\n",
    "class SaliencyTopFractionSquareOrFull:\n",
    "    def __init__(self, top_fraction=0.01, k=None):\n",
    "        \"\"\"\n",
    "        top_fraction: fraction of image pixels to consider as top salient points\n",
    "\n",
    "        For images with 231480 pixels, k=1000 seems good\n",
    "        1000 / 231480\n",
    "        \"\"\"\n",
    "        self.top_fraction = top_fraction\n",
    "        self.saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, img: Image.Image):\n",
    "        img_cv = np.array(img.convert('RGB'))[..., ::-1]\n",
    "        H, W = img_cv.shape[:2]\n",
    "        side = min(H, W)\n",
    "\n",
    "        # Compute saliency\n",
    "        success, saliency_map = self.saliency.computeSaliency(img_cv)\n",
    "        if not success:\n",
    "            return img\n",
    "\n",
    "        # Flatten saliency map and pick top fraction of pixels\n",
    "        flat = saliency_map.flatten()\n",
    "        k = self.k or max(1, int(self.top_fraction * flat.size))\n",
    "        topk_idx = np.argpartition(flat, -k)[-k:]\n",
    "        ys, xs = np.unravel_index(topk_idx, saliency_map.shape)\n",
    "\n",
    "        # --- Attempt to fit all top-k points in a square of size `side` ---\n",
    "        x_min, x_max = xs.min(), xs.max()\n",
    "        y_min, y_max = ys.min(), ys.max()\n",
    "        box_w = x_max - x_min + 1\n",
    "        box_h = y_max - y_min + 1\n",
    "\n",
    "        if box_w <= side and box_h <= side:\n",
    "            # Center the square on the top-k bounding box\n",
    "            cx = (x_min + x_max) // 2\n",
    "            cy = (y_min + y_max) // 2\n",
    "            left = cx - side // 2\n",
    "            top = cy - side // 2\n",
    "            # Clamp inside image\n",
    "            left = max(0, min(left, W - side))\n",
    "            top = max(0, min(top, H - side))\n",
    "            cropped = img_cv[top:top+side, left:left+side]\n",
    "            return Image.fromarray(cropped[..., ::-1])\n",
    "        else:\n",
    "            # Can't fit all points in the square → return full image\n",
    "            return img\n",
    "\n",
    "def resize_width_keep_aspect(img, target_width):\n",
    "    w, h = img.size\n",
    "    new_height = int(h * (target_width / w))\n",
    "    return img.resize((target_width, new_height), resample=Image.BICUBIC)\n",
    "\n",
    "def crop_top_square(img, top_offset=3, size=64):\n",
    "    return TF.crop(img, top=top_offset, left=0, height=size, width=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313118fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 937/938 [00:06<00:00, 136.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def mnist_tiler():\n",
    "    # Load MNIST dataset\n",
    "    mnist = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "    to_pil = ToPILImage()\n",
    "\n",
    "    # Shuffle all images\n",
    "    #images = [to_pil(img) for img, _ in mnist]\n",
    "    images = [img for img,_ in mnist]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Create output folder\n",
    "    os.makedirs(\"data/256x256/mnist_grid/tiles\", exist_ok=True)\n",
    "\n",
    "    # Parameters\n",
    "    canvas_size = 256\n",
    "    grid_size = 8\n",
    "    cell_size = canvas_size // grid_size\n",
    "    digit_size = 28\n",
    "    padding = (cell_size - digit_size) // 2\n",
    "\n",
    "    # Batch into groups of 64\n",
    "    for i in tqdm(range(0, len(images), 64)):\n",
    "        batch = images[i:i+64]\n",
    "        if len(batch) < 64:\n",
    "            break  # skip incomplete batch\n",
    "\n",
    "        # Create canvas with random background color\n",
    "        bg_color = tuple(random.randint(0, 255) for _ in range(3))\n",
    "        canvas = Image.new(\"RGB\", (canvas_size, canvas_size), bg_color)\n",
    "\n",
    "        for idx, digit in enumerate(batch):\n",
    "            row, col = divmod(idx, grid_size)\n",
    "            x = col * cell_size + padding\n",
    "            y = row * cell_size + padding\n",
    "\n",
    "            # Paste digit onto canvas\n",
    "            canvas.paste(digit.convert(\"RGB\"), (x, y))\n",
    "\n",
    "        # Save to disk\n",
    "        filename = f\"data/256x256/mnist_grid/tiles/{i//64 + 1:04d}.webp\"\n",
    "        canvas.save(filename, format=\"WEBP\")\n",
    "mnist_tiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantasy ./data/fantasy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12814/12814 [18:08<00:00, 11.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats_and_dogs ./data/cats_and_dogs/dataset/training_set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [01:15<00:00, 105.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pokemon ./data/pokemon_archive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2503/2503 [00:20<00:00, 122.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dragon utils/dataset_fetcher/datasets/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10287/10287 [02:21<00:00, 72.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yugioh data/yugioh/YuGiOhImages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13878/13878 [01:59<00:00, 116.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [02:41<00:00, 372.14it/s]\n"
     ]
    }
   ],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "    PadToSquare(size=256,color=(64,)),\n",
    "    transforms.Resize((HR, HR)),\n",
    "])\n",
    "\n",
    "yugioh_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: resize_width_keep_aspect(img, HR)),\n",
    "    transforms.Lambda(lambda img: crop_top_square(img, top_offset=3, size=HR)),\n",
    "])\n",
    "#hr_dataset = datasets.ImageFolder(root=data_root, transform=yugioh_transform)\n",
    "\n",
    "dragon_transform = transforms.Compose([\n",
    "    SaliencyTopFractionSquareOrFull(0.01), # dragon heads without much background\n",
    "    PadToSquare(),                         # crop to centered square\n",
    "    transforms.Resize((HR, HR)),               # resize to 64x64\n",
    "])\n",
    "\n",
    "hr_transform = transforms.Compose([\n",
    "    PadToSquare(),                        # crop to centered square\n",
    "    transforms.Resize((HR, HR)),               # resize to 64x64\n",
    "])\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "@lru_cache(10)\n",
    "def show_name(l):\n",
    "    print(l)\n",
    "\n",
    "def save_a_dataset(path,ds):\n",
    "    if os.path.exists(path):\n",
    "        print(\"already did {path}\")\n",
    "        return\n",
    "    for idx,(img, label) in enumerate(tqdm(ds)):\n",
    "        class_name = 'img'\n",
    "        class_dir = f\"{path}/{class_name}\"\n",
    "        os.makedirs(class_dir,exist_ok=True)\n",
    "        filename = f\"{idx:06d}.webp\"\n",
    "        out_path = os.path.join(class_dir, filename)\n",
    "        img.save(out_path, format=\"WEBP\", quality=50)\n",
    "\n",
    "for k,(path,checkpoint) in experiments.items():\n",
    "    print(k,path)\n",
    "    if k == 'yugioh':\n",
    "        src_ds = datasets.ImageFolder(root=path, transform=yugioh_transform)\n",
    "    elif k == 'mnist':\n",
    "        src_ds = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "    elif k == 'dragon':\n",
    "        src_ds = datasets.ImageFolder(root=path, transform=dragon_transform)\n",
    "    else:\n",
    "        src_ds = datasets.ImageFolder(root=path, transform=hr_transform)\n",
    "    path = f\"data/256x256/{k}\"\n",
    "    save_a_dataset(path,src_ds)\n",
    "\n",
    "    # \"fantasy\": (\"./data/fantasy\", \"checkpoints/UNet128_2025-09-12_08-24-21_fantasy.pth\"),\n",
    "    # \"cats_and_dogs\": (\"./data/cats_and_dogs/dataset/training_set\", \"checkpoints/UNet128_2025-09-08_17-57-30_cats_and_dogs.pth\"),\n",
    "    # \"pokemon\": (\"./data/pokemon_archive\",\"UNet128_2025-09-12_11-03-12_pokemon.pth\"),\n",
    "    # \"dragon\": (\"utils/dataset_fetcher/datasets/\", \"checkpoints/UNet128_2025-09-12_19-47-15_dragon.pth\"),\n",
    "    # \"yugioh\": (\"data/yugioh/YuGiOhImages\",None),\n",
    "    # \"mnist\": (None,None),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd40b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73513b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edm_diffusion_vibe_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
