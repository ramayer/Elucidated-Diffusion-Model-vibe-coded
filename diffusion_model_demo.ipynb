{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ccbd02",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Model (DDPM) Implementation\n",
    "\n",
    "This notebook implements the diffusion model described in the paper (2206.00364v2.pdf). It will guide you through the process of building, training, and sampling from a DDPM using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ba9c7",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "We will use PyTorch, torchvision, numpy, matplotlib, and tqdm. If running locally, ensure these packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43048b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in a new environment)\n",
    "# !pip install torch torchvision matplotlib tqdm numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4c915",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Dataset\n",
    "\n",
    "We will use the MNIST dataset for demonstration. The images will be normalized to [-1, 1] as required by most diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2. - 1.)  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Visualize a batch\n",
    "examples = next(iter(train_loader))[0][:8]\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
    "for i, img in enumerate(examples):\n",
    "    axes[i].imshow(img.squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3e8ab",
   "metadata": {},
   "source": [
    "## 3. Define the Diffusion Model Architecture\n",
    "\n",
    "We will use a simple U-Net-like architecture suitable for MNIST. For more complex datasets, a deeper U-Net or transformer-based model may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple U-Net-like model for MNIST\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Conv2d(32, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # t is the timestep, can be embedded and concatenated if desired\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        m = self.middle(d2)\n",
    "        u2 = self.up2(m)\n",
    "        u1 = self.up1(u2)\n",
    "        out = self.out(u1)\n",
    "        return out\n",
    "\n",
    "model = SimpleUNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bb5bb",
   "metadata": {},
   "source": [
    "## 4. Implement the Forward Diffusion Process\n",
    "\n",
    "The forward process gradually adds Gaussian noise to the data over a fixed number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward diffusion process\n",
    "T = 200  # Number of diffusion steps\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "betas = torch.linspace(beta_start, beta_end, T)\n",
    "alphas = 1. - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    sqrt_alpha_bar = alpha_bars[t].sqrt().view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alpha_bar = (1 - alpha_bars[t]).sqrt().view(-1, 1, 1, 1)\n",
    "    return sqrt_alpha_bar * x_start + sqrt_one_minus_alpha_bar * noise\n",
    "\n",
    "# Visualize noisy images at different timesteps\n",
    "x = examples[:4]\n",
    "timesteps = torch.tensor([0, T//4, T//2, T-1])\n",
    "noisy_imgs = [q_sample(x, torch.full((x.size(0),), t, dtype=torch.long)) for t in timesteps]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 2))\n",
    "for i, img in enumerate(noisy_imgs):\n",
    "    axes[i].imshow(img[0].squeeze().numpy(), cmap='gray')\n",
    "    axes[i].set_title(f\"t={timesteps[i].item()}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a2a51",
   "metadata": {},
   "source": [
    "## 5. Implement the Reverse (Denoising) Process\n",
    "\n",
    "The reverse process uses the model to predict the noise at each timestep and denoise the image step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bff6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse (denoising) process for sampling\n",
    "def p_sample(model, x, t):\n",
    "    beta = betas[t]\n",
    "    sqrt_one_minus_alpha_bar = (1 - alpha_bars[t]).sqrt()\n",
    "    sqrt_recip_alpha = (1. / alphas[t]).sqrt()\n",
    "    model_mean = sqrt_recip_alpha * (x - beta / sqrt_one_minus_alpha_bar * model(x, torch.tensor([t])))\n",
    "    if t > 0:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + beta.sqrt() * noise\n",
    "    else:\n",
    "        return model_mean\n",
    "\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.randn(shape, device=device)\n",
    "    for t in reversed(range(T)):\n",
    "        x = p_sample(model, x, t)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407711ca",
   "metadata": {},
   "source": [
    "## 6. Train the Diffusion Model\n",
    "\n",
    "Set up the training loop, loss function, and optimizer. The model is trained to predict the noise added at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a24993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for DDPM\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "epochs = 1  # For demonstration; increase for better results\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(train_loader)\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "        t = torch.randint(0, T, (x.size(0),), device=device).long()\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = q_sample(x, t, noise)\n",
    "        noise_pred = model(x_noisy, t)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c8d17",
   "metadata": {},
   "source": [
    "## 7. Generate Samples with the Trained Model\n",
    "\n",
    "Use the trained model to generate new samples by running the reverse diffusion process starting from random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    samples = p_sample_loop(model, (8, 1, 28, 28)).cpu()\n",
    "    samples = (samples + 1) / 2  # Rescale to [0, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
    "for i, img in enumerate(samples):\n",
    "    axes[i].imshow(img.squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13fdf49",
   "metadata": {},
   "source": [
    "## 8. Visualize Generated Samples\n",
    "\n",
    "Display the generated samples to assess the performance of the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e68b7",
   "metadata": {},
   "source": [
    "# Elucidated Diffusion Model (EDM) Implementation\n",
    "\n",
    "This section implements the EDM as described in the paper (2206.00364v2.pdf), including its unique noise schedule, loss weighting, and sampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d97cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDM noise schedule, loss weighting, and sampling procedure\n",
    "# Reference: 2206.00364v2.pdf (Karras et al., 2022)\n",
    "\n",
    "# EDM uses a continuous noise schedule and loss weighting\n",
    "# We use sigma_min, sigma_max, and rho as in the paper\n",
    "sigma_min = 0.002\n",
    "sigma_max = 80\n",
    "rho = 7\n",
    "\n",
    "# EDM noise schedule (sampling)\n",
    "def edm_sigma_schedule(t):\n",
    "    return (sigma_max ** (1/rho) + t * (sigma_min ** (1/rho) - sigma_max ** (1/rho))) ** rho\n",
    "\n",
    "# EDM loss weighting\n",
    "# w(t) = (sigma^2 + 1) / (sigma^2)\n",
    "def edm_loss_weight(sigma):\n",
    "    return (sigma ** 2 + 1) / (sigma ** 2)\n",
    "\n",
    "# Example: plot the EDM noise schedule\n",
    "ts = torch.linspace(0, 1, 100)\n",
    "sigmas = edm_sigma_schedule(ts)\n",
    "plt.plot(ts.numpy(), sigmas.numpy())\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('sigma(t)')\n",
    "plt.title('EDM Noise Schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDM training loop (for MNIST, using the same U-Net model)\n",
    "# This uses the EDM noise schedule and loss weighting\n",
    "\n",
    "edm_epochs = 1  # For demonstration; increase for better results\n",
    "model_edm = SimpleUNet().to(device)\n",
    "optimizer_edm = optim.Adam(model_edm.parameters(), lr=2e-4)\n",
    "\n",
    "for epoch in range(edm_epochs):\n",
    "    pbar = tqdm(train_loader)\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "        t = torch.rand(x.size(0), device=device)  # Uniform in [0, 1]\n",
    "        sigma = edm_sigma_schedule(t)\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = x + sigma.view(-1, 1, 1, 1) * noise\n",
    "        noise_pred = model_edm(x_noisy, (sigma * 1000).long())  # Pass scaled sigma as timestep\n",
    "        weight = edm_loss_weight(sigma).view(-1, 1, 1, 1)\n",
    "        loss = ((noise_pred - noise) ** 2 * weight).mean()\n",
    "        optimizer_edm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_edm.step()\n",
    "        pbar.set_description(f\"EDM Epoch {epoch+1} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDM sampling procedure (ancestral sampling)\n",
    "def edm_ancestral_sampling(model, num_steps=18, batch_size=8, img_shape=(1, 28, 28)):\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.randn((batch_size,) + img_shape, device=device) * sigma_max\n",
    "    sigmas = edm_sigma_schedule(torch.linspace(1, 0, num_steps, device=device))\n",
    "    for i in range(num_steps):\n",
    "        sigma = sigmas[i]\n",
    "        sigma_next = sigmas[i+1] if i+1 < num_steps else torch.tensor(sigma_min, device=device)\n",
    "        c_in = 1 / (sigma ** 2 + 1).sqrt()\n",
    "        c_out = sigma_next / sigma\n",
    "        c_noise = sigma * 1000  # Scale for timestep embedding\n",
    "        noise_pred = model(c_in * x, torch.full((batch_size,), c_noise, device=device).long())\n",
    "        d = (x - sigma * noise_pred) / sigma\n",
    "        x = x + (sigma_next - sigma) * d\n",
    "        if sigma_next > 0:\n",
    "            x = x + torch.randn_like(x) * (sigma_next ** 2 - sigma ** 2).sqrt()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize samples from the trained EDM model\n",
    "model_edm.eval()\n",
    "with torch.no_grad():\n",
    "    edm_samples = edm_ancestral_sampling(model_edm, batch_size=8, img_shape=(1, 28, 28)).cpu()\n",
    "    edm_samples = (edm_samples + 1) / 2  # Rescale to [0, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
    "for i, img in enumerate(edm_samples):\n",
    "    axes[i].imshow(img.squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('EDM Generated Samples')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
